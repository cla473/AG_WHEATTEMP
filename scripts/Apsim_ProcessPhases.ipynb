{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Growth Phases "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks at the SQLite Databases generated by Apsim X (Next Gen) for 109 Wheat varieties, 10 differing sow dates for  57,434 locations/sites across Australia.  \n",
    "Each database file covers one (1) site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import the required libraries\n",
    "import sys\n",
    "import os\n",
    "import datetime\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the working directories\n",
    "apsim_sourcedir = \"/OSM/CBR/AG_WHEATTEMP/source\"\n",
    "apsim_outfiledir = \"/OSM/CBR/AG_WHEATTEMP/work/output\"\n",
    "metfile_sourcedir = \"/OSM/CBR/AG_WHEATTEMP/work/ApsimNG-test/APSIM_run/met\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbfile_df = pd.DataFrame(columns=['filename'])\n",
    "dbfile_df.filename = sorted(apsim_sourcedir+'/'+f for f in os.listdir(apsim_sourcedir) if f.endswith('.db'))\n",
    "print(dbfile_df.head())\n",
    "dbname = dbfile_df.filename[4]\n",
    "print(dbname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#these are required for calculations of thermal time\n",
    "num3hr = int(24 / 3)\n",
    "#print(\"num3hr: \", num3hr)    \n",
    "t_range_fract = []\n",
    "\n",
    "# pre calculate t_range_fract for speed reasons\n",
    "for period in range(num3hr):\n",
    "    calcValue = 0.92105 \\\n",
    "                + 0.1140 * period \\\n",
    "                - 0.0703 * math.pow(period, 2) \\\n",
    "                + 0.0053 * math.pow(period, 3)\n",
    "    t_range_fract.append(calcValue)\n",
    "\n",
    "#print(\"t_range_fract: \", t_range_fract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linint_3hrly_Temperature(tmax, tmin, xp, fp):\n",
    "    '''\n",
    "    Eight interpolations of the air temperature are calculated using \n",
    "    a three-hour correction factor.\n",
    "    For each air three-hour air temperature, a value is calculated.\n",
    "    The eight three-hour estimates are then averaged to obtain the daily value.\n",
    "    '''\n",
    "\n",
    "    #Local Variables\n",
    "    tot = 0.0            #sum_of of 3 hr interpolations\n",
    "    \n",
    "    for period in range(1, num3hr):\n",
    "        #get mean temperature for 3 hr period (oC)\n",
    "        #tmean_3hour = temp_3hr(tmax, tmin, period)\n",
    "        tmean_3hour = tmin + (t_range_fract[period-1] * (tmax - tmin))\n",
    "        #tot = tot + ttFn.valueIndexed(tmean_3hour)\n",
    "        tot = tot + np.interp(tmean_3hour, xp,fp)\n",
    "        \n",
    "        #print(\"tmean_3hour: \", tmean_3hour, \" - tot: \", tot)\n",
    "\n",
    "    return tot / num3hr;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_ApsimWeather(filename, lat):\n",
    "    '''\n",
    "    Reads an apsim weather ('.met') file, removes the header information,\n",
    "    calculates and adds a date column (based on year and day), and the\n",
    "    average temperature (based on maxt and mint).\n",
    "    '''\n",
    "    import math\n",
    "\n",
    "    xp = [0, 26, 37]\n",
    "    fp = [0, 26, 0]\n",
    "    \n",
    "    lineNo = 0\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            lineNo = lineNo + 1\n",
    "            if line.startswith('year'):\n",
    "                break;\n",
    "\n",
    "    # return the data using the starting line no (determined above)\n",
    "    # original column names=['year','day', 'radn', 'maxt', 'mint', 'rain']\n",
    "    metData = pd.read_table(filename, sep='\\s+', header=None, skiprows=lineNo+1,\n",
    "                            names=['year','dayofYear', 'radiation', 'maxTemp', 'minTemp', 'rain'])\n",
    "    \n",
    "    # add the calculated columns\n",
    "    metData['runDate'] = pd.to_datetime(metData['year'].astype(str) + \" \" + metData['dayofYear'].astype(str), format=\"%Y %j\")\n",
    "\n",
    "    # this may need to be the thermal time, not just average temp\n",
    "    metData['avgTemp'] = (metData['maxTemp'] + metData['minTemp']) / 2\n",
    "\n",
    "    #convert the radiation from MJ/m2/day to Photosynthetically active radiation (PAR)\n",
    "    metData['PARIO'] = metData['radiation'] * 0.47\n",
    "\n",
    "    #convert the measurement unit for the radiation from MJ/m2/day to J/m2/day\n",
    "    metData['radnJ'] = metData['radiation'] * 1000000\n",
    "\n",
    "\n",
    "    # calculation the day length\n",
    "    radians = math.pi/180\n",
    "    lambdaRadians = float(latitude) * radians\n",
    "\n",
    "    sinLAT = math.sin(lambdaRadians)\n",
    "    cosLAT = math.cos(lambdaRadians)\n",
    "    sinDMC = math.sin(radians * 23.45)\n",
    "\n",
    "    #print(\"radians: \", radians)\n",
    "    #print(\"lambdaRadians: \", lambdaRadians)\n",
    "    #print(\"sinLAT: \", sinLAT)\n",
    "    #print(\"cosLAT: \", cosLAT)\n",
    "    #print(\"sinDMC: \", sinDMC)    \n",
    "    \n",
    "    metData['sinDEC'] = -sinDMC * np.cos(2 * math.pi * (metData['dayofYear'] + 10) / 365)\n",
    "    metData['cosDEC'] = np.sqrt(1 - (metData['sinDEC'] * metData['sinDEC']))\n",
    "    metData['a'] = sinLAT * metData['sinDEC']\n",
    "    metData['b'] = cosLAT * metData['cosDEC']\n",
    "\n",
    "    metData['daylength'] = 12 * (1 + (2 / math.pi) * np.arcsin(metData['a']/metData['b']))\n",
    "\n",
    "    # calculate the Fraction Disfused Radiation (FDR)\n",
    "    metData['hour'] = np.mod(metData['dayofYear'], 1) * 24\n",
    "    metData['sinB'] = metData['a'] + metData['b'] * np.cos(2 * math.pi * (metData['hour'] - 12) / 24)\n",
    "    metData['SC'] = 1367 * (1 + 0.033 * np.cos(2 * math.pi * (metData['dayofYear'] - 10) / 365))\n",
    "    metData['sinINT'] = metData['a'] * metData['daylength'] + (24 * metData['b'] / math.pi) * \\\n",
    "                        np.cos((math.pi / 2) * ((metData['daylength'] / 12) - 1))\n",
    "\n",
    "    metData['Ta'] = metData['radnJ'] / (metData['sinINT'] * 3600 * metData['SC'])\n",
    "    metData['FDR'] = metData['Ta'] * -1.6 +1.32\n",
    "\n",
    "    # calculate the Evapotranspiration\n",
    "    metData['vpsl'] = 238.102 * 17.32491 * ((metData['minTemp'] + metData['maxTemp']) /2) / \\\n",
    "                      (((metData['minTemp'] + metData['maxTemp']) / 2) + 238.102) ** 2\n",
    "    metData['ETpt'] = 1.26 * (metData['radnJ']  * (metData['vpsl'] / (metData['vpsl'] + 0.067))) / 2454000\n",
    "\n",
    "    metData['ApsimTT'] = metData.apply(lambda x: linint_3hrly_Temperature(x['maxTemp'], x['minTemp'], xp, fp), axis=1)\n",
    "    \n",
    "\n",
    "    # sort the columns to be a little more logical\n",
    "    #cols=['year', 'dayofYear', 'runDate', 'dayLength', 'maxTemp', 'minTemp', 'avgTemp', 'ApsimTT', 'rain', \\\n",
    "    #      'PARIO', 'fracDiffusedRadn', 'vpsl', 'ETpt']\n",
    "    #metData = metData[cols]    \n",
    "    \n",
    "    return metData\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_simulation_details(dbname):\n",
    "    '''\n",
    "    Opens the specified SQL Database and extracts the 'Name' details from the Simulation\n",
    "    Table, and splits it to Simulation ID, Longitude, Latitude, Variety, SowDate, and\n",
    "    returns ad dataframe.\n",
    "    '''\n",
    "\n",
    "    # connect to the Database\n",
    "    con = sqlite3.connect(dbname)\n",
    "    cur = con.cursor()\n",
    "\n",
    "    # get contents of the _Simulation Table\n",
    "    strSql = \"SELECT ID as SimulationID, Name FROM _Simulations\"\n",
    "    dfSim = pd.read_sql_query(strSql, con, index_col = 'SimulationID')\n",
    "\n",
    "    # split the 'Name' field into long, lat, variety and sowdate columns\n",
    "    dfSim[['long','lat','variety','sowdate']] = \\\n",
    "    dfSim['Name'].str.extract(\"^(?P<long>\\d+)_(?P<lat>-?\\d+)_(?P<variety>\\S+)_(?P<sowdate>\\d+-\\S+)$\", expand=True)\n",
    "\n",
    "    # format the columns\n",
    "    pd.options.display.float_format = '{:,.2f}'.format\n",
    "    dfSim['long'] = dfSim['long'].astype(float) / 100\n",
    "    dfSim['lat'] = dfSim['lat'].astype(float) / 100\n",
    "\n",
    "    # create a SimId column (as the original SimulationID is now an index column)\n",
    "    dfSim['SimID'] = dfSim.index \n",
    "\n",
    "    return dfSim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_report_details(dbname):\n",
    "    '''\n",
    "    Opens the specified SQL Database and extracts the details from the Report\n",
    "    Table, formats the columns correctly and returns a dataframe\n",
    "    '''\n",
    "\n",
    "    # connect to the Database\n",
    "    con = sqlite3.connect(dbname)\n",
    "    cur = con.cursor()\n",
    "\n",
    "    # get contents of the Report Table\n",
    "    #strSql = \"SELECT SimulationID, substr([Clock.Today], 1, 10) as runDate, \\\n",
    "    #      [Wheat.Leaf.LAI] as LAI, [Wheat.AboveGround.Wt] as Biomass, \\\n",
    "    #      [Wheat.Grain.Wt] as Yield, [Wheat.Phenology.Zadok.Stage] as ZadokStage, \\\n",
    "    #      [Wheat.WaterSupplyDemandRatio] as WaterSupplyDemandRatio, \\\n",
    "    #      [Wheat.Root.NUptake] as RootNUptake, [Wheat.Leaf.Fn] as LeafFn \\\n",
    "    #      FROM Report \\\n",
    "    #      ORDER BY SimulationID, runDate\"\n",
    "    \n",
    "    #Need to exclude consider CUTOFFS\n",
    "    #Need to use DISTINCT AS THERE SEEMS TO BE SOME DUPLICATIONS IN TO DATA\n",
    "\n",
    "    #do not need all of the columns, will cut this down from the get go\n",
    "    strSql = \"SELECT DISTINCT SimulationID, substr([Clock.Today], 1, 10) as runDate, \\\n",
    "          [Wheat.Leaf.LAI] as LAI, [Wheat.AboveGround.Wt] as Biomass, \\\n",
    "          [Wheat.Grain.Wt] as Yield, [Wheat.Phenology.Zadok.Stage] as ZadokStage, \\\n",
    "          [Wheat.WaterSupplyDemandRatio] as WaterSupplyDemandRatio \\\n",
    "          FROM Report \\\n",
    "          ORDER BY SimulationID, runDate\"    \n",
    "    #print(datetime.datetime.now())\n",
    "    dfReport = pd.read_sql_query(strSql, con, index_col=\"SimulationID\" )\n",
    "    #print(datetime.datetime.now())\n",
    "\n",
    "    # format the date columns\n",
    "    dfReport['runDate'] = pd.to_datetime(dfReport['runDate'], format=\"%Y-%m-%d\")\n",
    "\n",
    "    # create the SimId column\n",
    "    dfReport['SimID'] = dfReport.index\n",
    "\n",
    "    return dfReport\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_filename(dbname):\n",
    "    '''\n",
    "    Takes the full path and filename for the database file, and creates the filename\n",
    "    that is used for the weather file, and to save the output.\n",
    "\n",
    "    Note:  cannot use the db filename as it doesn't have the long & lat that we require\n",
    "           need to manipulate the filename to add the underscrore '_' char\n",
    "           ONLY need to allow for negative (south) latitudes\n",
    "    '''\n",
    "    filename = os.path.basename(dbname)\n",
    "    filename = os.path.splitext(filename)[0]\n",
    "    nameparts = filename.split('-')\n",
    "    filename = nameparts[0] + '_-' + nameparts[1]\n",
    "    lat = '-' + nameparts[1]\n",
    "\n",
    "    return filename, lat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_weather_details(filename, latitude):\n",
    "    '''\n",
    "    Retrieves the weather data for the location (long,lat) specified in the dbname,\n",
    "    formats the data, and returns a dataframe\n",
    "    '''\n",
    "\n",
    "    fullfilename = metfile_sourcedir + \"/c_\" + filename + \".met\"\n",
    "    dfWeather = read_ApsimWeather(fullfilename, latitude)\n",
    "\n",
    "    return dfWeather\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the data\n",
    "The following forms a single function:  def process_Apsim_dbfile(dbname):  \n",
    "#### NOTE:  dbname is defined at the top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"processing file: \", dbname)\n",
    "print(\"started at \", datetime.datetime.now())\n",
    "filename, latitude = get_filename(dbname)\n",
    "\n",
    "print(\"filename: \", filename)\n",
    "print(\"latitude: \", latitude)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the weather data from the weather '.met' file\n",
    "dfWeather = get_weather_details(filename, latitude)\n",
    "print(dfWeather.shape)\n",
    "print(dfWeather.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this is so that we can check the calculations and see if everything is correct\n",
    "#outfilename = apsim_outfiledir + \"/\" + filename + \"_calcs.csv\"\n",
    "#dfWeather.to_csv(outfilename, encoding='utf-8', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this will be part of the get_weather_details functionality, but is commented out so that I can check things\n",
    "cols=['year', 'dayofYear', 'runDate', 'daylength', 'maxTemp', 'minTemp', 'avgTemp', 'ApsimTT', 'rain', \\\n",
    "      'PARIO', 'FDR', 'vpsl', 'ETpt']\n",
    "dfWeather = dfWeather[cols] \n",
    "dfWeather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the Simulation Details from the DB._Sumulation table\n",
    "dfSim = get_simulation_details(dbname) \n",
    "print(dfSim.shape)\n",
    "print(dfSim.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the Details from the DB.Report table\n",
    "dfReport = get_report_details(dbname)\n",
    "print(dfReport.shape)\n",
    "print(dfReport.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the data with the Simulation details, so that we can get the sow date\n",
    "# and filter it again\n",
    "dfCombined = dfReport.merge(dfSim, on=\"SimID\", how='left')\n",
    "dfCombined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a sowing date (with current year)\n",
    "dfCombined['sowingdate'] = dfCombined['sowdate'] + '-' + dfCombined['runDate'].dt.year.map(str)\n",
    "dfCombined['sowingdate'] = pd.to_datetime(dfCombined['sowingdate'], format=\"%d-%b-%Y\")\n",
    "dfCombined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the data from the Report details that we are not going to be looking at\n",
    "# ie, any data prior to our sow date for each year (should reduce size of data considerably)\n",
    "dfCombined = dfCombined[(dfCombined['runDate'] >= dfCombined['sowingdate'])] \n",
    "dfCombined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#re-order and filter columns\n",
    "cols = ['SimID', 'long', 'lat', 'variety', 'sowdate', 'sowingdate', 'runDate', 'LAI', 'Biomass', 'Yield', \\\n",
    "        'ZadokStage', 'WaterSupplyDemandRatio']\n",
    "dfCombined = dfCombined[cols] \n",
    "dfCombined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the report data with the weather data\n",
    "dfCombined = dfCombined.merge(dfWeather, on='runDate', how='left')\n",
    "dfCombined\n",
    "# filter the data based on the information we want\n",
    "#filterCols = ['SimID', 'runDate', 'ZadokStage', 'avgTemp']\n",
    "#dfSubData = dfCombined[filterCols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create bins for the phases\n",
    "bins = [0, 7, 10, 31, 39, 65, 71, 87, 89, 90]\n",
    "group_names = ['01_Germinating', '02_Emerging', '03_Vegetative', '04_StemElongation',\n",
    "               '05_EarlyReproductive', '06_GrainSet', '07_GrainFilling', '08_Maturing', '09_Ripening']\n",
    "dfCombined['phases'] = pd.cut(dfCombined['ZadokStage'], bins, labels=group_names)\n",
    "dfCombined\n",
    "\n",
    "#might need to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#==============================================================\n",
    "# TAKE A COPY OF THE ABOVE DATASET SO IT DOESN'T GET MESSED UP\n",
    "#==============================================================\n",
    "#dfData = dfCombined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to work out the Ripening phase\n",
    "# starts where 90 is maturity (start of ripening stage)\n",
    "dfCombined['cumApsimTT'] = dfCombined.groupby(by=['SimID','sowingdate'])['ApsimTT'].cumsum()\n",
    "dfCombined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These are cumulative calculations for each phase\n",
    "dfCombined['cumPhaseAvgTemp'] = dfCombined.groupby(by=['SimID','sowingdate', 'phases'])['avgTemp'].cumsum()\n",
    "dfCombined['cumPhaseApsimTT'] = dfCombined.groupby(by=['SimID','sowingdate', 'phases'])['ApsimTT'].cumsum()\n",
    "dfCombined['cumPhaseRain'] = dfCombined.groupby(by=['SimID','sowingdate', 'phases'])['rain'].cumsum()\n",
    "dfCombined['cumPhaseETpt'] = dfCombined.groupby(by=['SimID','sowingdate', 'phases'])['ETpt'].cumsum()\n",
    "dfCombined['cumPhasePARIO'] = dfCombined.groupby(by=['SimID','sowingdate', 'phases'])['PARIO'].cumsum()\n",
    "dfCombined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#output a subset so I can look at the data\n",
    "cols = ['SimID', 'long', 'lat', 'variety', 'sowdate', 'sowingdate', 'runDate', 'year', 'dayofYear', \\\n",
    "        'daylength', 'ZadokStage', 'phases', 'maxTemp', 'minTemp', 'avgTemp', 'cumPhaseAvgTemp', \\\n",
    "        'ApsimTT', 'cumApsimTT', 'cumPhaseApsimTT', 'rain', 'cumPhaseRain', 'PARIO', 'cumPhasePARIO', \\\n",
    "        'FDR', 'vpsl', 'ETpt', 'cumPhaseETpt', 'LAI', 'Biomass', 'Yield','WaterSupplyDemandRatio']\n",
    "dfCombined = dfCombined[cols]\n",
    "dfCombined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subData = dfCombined.loc[dfCombined['SimID'].isin([2,3,4])]\n",
    "outfilename = apsim_outfiledir + \"/sim123_cum_\" + filename + \".csv\"\n",
    "subData.to_csv(outfilename, encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filter the data on the tempavgTemp column\n",
    "newData = dfSubData[dfSubData['tempavgTemp'] > 0]\n",
    "#newData1 = newData.groupby(['SimID','sowdate'])['TTAfterSowing'].max().reset_index()\n",
    "newData1 = newData.groupby(['SimID','sowingdate'])['TTAfterSowing'].max().reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#newData2 = newData1.groupby(['SimID','sowdate'])['TTAfterSowing'].mean().reset_index()\n",
    "newData2 = newData1.groupby(['SimID','sowingdate'])['TTAfterSowing'].mean().reset_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Need to determine the phases: \n",
    "•\tlength (no of days in phase), minTemp, maxTemp, and avgTemp,  \n",
    "•\tthe cumulativeAvgTemp at the end of each phase, (needs to be called TTAfterSowing)  \n",
    "•\tthe counts for number of day where temperatures are below or above specified values (refer to SIP_Temp discussion.docs for further details)  \n",
    "•\tthe average and cumulative rainfall   \n",
    "•\taverage radiation, cumulative radiation  \n",
    "•\taverage watersupplydemandratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# eliminate those records where the phase is Nan\n",
    "#dfSubData.dropna(subset=['phases'], inplace=True)\n",
    "#dfSubData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create a function to get the stats for group\n",
    "def get_stats(group):\n",
    "    return {'min': group.min(), 'max': group.max(), 'mean': group.mean(), 'count': group.count()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#should I add a column that can be a concatinated key\n",
    "dfSubData['SimIdSowingdatePhase'] = dfSubData['SimId'].map(str) + \\ \n",
    "    dfSubData['sowingdate'].map(str) + dfSubData['phases']\n",
    "\n",
    "#apply the get_stats function to each phase bin\n",
    "dfSubData['ZadokStage'].groupby(dfSubData['SimIdSowingdatePhase']).apply(get_stats).unstack()\n",
    "dfSubData,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#how many days in each of the phases\n",
    "newData1.groupby(['SimIdSowingdatePhase'].count().reset_index()\n",
    "\n",
    "#what is the minimimum temperature for the phase\n",
    "newData1.groupby(['SimIdSowingdatePhase'])['minTemp'].min().reset_index()\n",
    "#what is the maximum temperature for the phase\n",
    "newData1.groupby(['SimIdSowingdatePhase'])['maxTemp'].max().reset_index()\n",
    "\n",
    " newData1.groupby(['SimIdSowingdatePhase'])['biomass'].max().reset_index()\n",
    "\n",
    "#what is the average of average temp for the phase\n",
    "newData1.groupby(['SimIdSowingdatePhase'])['avgTemp'].mean().reset_index()\n",
    "\n",
    "#what is the average of average daily radiation for the phase\n",
    "#newData1.groupby(['SimIdSowingdatePhase'])['radn'].mean().reset_index()\n",
    "\n",
    "#what is the cumulative rainfall for each phase\n",
    "newData1.groupby(by=['SimIdSowingdatePhase'])['rain'].cumsum()\n",
    "\n",
    "newData1.groupby(by=['SimIdSowingdatePhase'])['ETpt'].cumsum()\n",
    "\n",
    "newData1.groupby(by=['SimIdSowingdatePhase'])['PARIO'].mean().reset_index()\n",
    "newData1.groupby(by=['SimIdSowingdatePhase'])['PARIO'].cumsum()\n",
    "\n",
    "newData1.groupby(by=['SimIdSowingdatePhase'])['PQDay'].mean.reset_index()\n",
    "newData1.groupby(by=['SimIdSowingdatePhase'])['daylengh'].mean.reset_index()\n",
    "\n",
    "#need to get the values for the last day in each phase:\n",
    "startdate, enddate, DAS, LAI, Biomass, Yield\n",
    "                \n",
    "                 \n",
    "#what is the average of average watersupplydemandratio for the phase\n",
    "newData1.groupby(['SimIdSowingdatePhase'])['watersupplydemandratio'].mean().reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get the counts of various miniumum temperatures\n",
    "newData1[newData1['minTemp'] <= 0].groupby['SimIdSowingdatePhase'].count()\n",
    "newData1[newData1['minTemp'] <= -1].groupby['SimIdSowingdatePhase'].count()\n",
    "newData1[newData1['minTemp'] <= -2].groupby['SimIdSowingdatePhase'].count()\n",
    "newData1[newData1['minTemp'] <= -3].groupby['SimIdSowingdatePhase'].count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get the counts of various maximum temperatures\n",
    "newData1[newData1['maxTemp'] >= 30].groupby['SimIdSowingdatePhase'].count()\n",
    "newData1[newData1['maxTemp'] >= 32].groupby['SimIdSowingdatePhase'].count()\n",
    "newData1[newData1['maxTemp'] >= 34].groupby['SimIdSowingdatePhase'].count()\n",
    "newData1[newData1['maxTemp'] >= 36].groupby['SimIdSowingdatePhase'].count()\n",
    "newData1[newData1['maxTemp'] >= 38].groupby['SimIdSowingdatePhase'].count()\n",
    "newData1[newData1['maxTemp'] >= 40].groupby['SimIdSowingdatePhase'].count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# need to add back in the longitude, latitude, variety from dfSim\n",
    "newData2 = newData2.merge(dfSim, on=['SimID', 'sowdate'], how='left')\n",
    "filterCols = ['SimID', 'long', 'lat', 'variety', 'sowdate', 'TTAfterSowing']\n",
    "newData2 = newData2[filterCols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outfilename = apsim_outfiledir + \"/\" + filename + \"_zadok.csv\"\n",
    "newData2.to_csv(outfilename, encoding='utf-8', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
